# -*- coding: utf-8 -*-
"""MSPA_Project_phase_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kREBOgD_eivIhULdqtEnnF2tqQpijqyT

<a href="https://colab.research.google.com/github/Abhishekravindran/Image-and-sentiment-end-to-end-detection/blob/main/MSPA_Project_phase_1.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

### Media Industry and Computer Vision

There are quite a few applications of Deep Learning and Computer Vision in Media Industry. 



*   Recommendation
*   Search
*   Promo Generation
*   Content Scheduling

And many more applications



From video contents, identifying useful and relevant information in a structured format is important for:


*   Allowing Customers to searching relevant
*   Analytics on Video Contents and User Viewership

Finding Meta Tags (information about video contents) is traditional a manual process. The meta tags can be extracted based on the video and audio contents. 

In this blog, we will the video content and extract relevant information for India TV serials. 

There are plenty of shows for similar target audience and content creation teams are looking for ways to find key insights and improve the contents.

### Plan for the Blog


*   Objectives
*   Video Contents and Data
*   Face Detection and Extraction
*   Labelling
*   Face Recogniation Models
*   Model Validation
*   Model Deployment and Application

### Objective

### Video Contents and Data

Description of the shows and details of the characters
"""

from google.colab import drive
drive.mount("/content/drive")

import os
os.mkdir('/content/sample_data/image')

import cv2
import os
#passing the path of the video to a variable 
vidcap = cv2.VideoCapture('/content/drive/MyDrive/Copy of Imlie 22nd May 2021.mp4 _0.mp4')
path='/content/sample_data/image'
def getFrame(sec):

    vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)#skipping to the first second in the video
    hasFrames,image = vidcap.read()
    if hasFrames: #condition check with frames 
        cv2.imwrite(os.path.join(path,"img"+str(count)+".jpg"), image)     # save frame as JPG format
    return hasFrames
sec = 0
frameRate = 1.0 #it will capture image in each 1 second
count=1
success = getFrame(sec)
while success:
    count = count + 1
    sec = sec + frameRate
    sec = round(sec, 2)
    success = getFrame(sec)

import os
print(os.listdir('/content/sample_data/image'))

"""import os

def get_filepaths(directory):
   
    
    file_paths = []  # List which will store all of the full filepaths.

    
    for root, directories, files in os.walk(directory):
        for filename in files:
            # Join the two strings in order to form the full filepath.
            filepath = os.path.join(root, filename)
            file_paths.append(filepath)  # Add it to the list.

    return file_paths  # Self-explanatory.

  
full_file_paths = get_filepaths("/content/sample_data/image")

#full_file_paths

def get_paths():
  paths=[os.path.join('/content/sample_data/image',f)for f in os.listdir('/content/sample_data/image')]
  return paths
full_file_paths=get_paths()
len(full_file_paths)
#full_file_paths

full_file_paths

len(full_file_paths),full_file_paths[0]

!pip install mtcnn

import matplotlib.pyplot as plt

from matplotlib import pyplot
from matplotlib.patches import Rectangle
#from matplotlib.patches import Circle
from mtcnn.mtcnn import MTCNN
import cv2

"""### Face Detection and Extraction

Refrence about mtcnn

https://medium.com/@iselagradilla94/multi-task-cascaded-convolutional-networks-mtcnn-for-face-detection-and-facial-landmark-alignment-7c21e8007923
"""

detector=MTCNN()#fitting the mtcnn model

from google.colab.patches import cv2_imshow

def extract_face(image,resize=(244,244)):
  image=cv2.imread(image)

  faces=detector.detect_faces(image)#detecting the faces from the image which mtcnn is doing in background 
  face_images = []#creating list to store the diffrent faces returned

  for face in faces:
    x1,y1,width,height=faces[0]['box'] #a image will have 4 components from its detected point i.e  
    x2,y2=x1+width,y1+height

    face_boundary=[]

    face_boundary=image[y1:y2,x1:x2]

    face_image=cv2.resize(face_boundary,resize)

  return face_image

import os
os.mkdir('/content/sample_data/face_extracted')

path='/content/sample_data/face_extracted'
count=0
for i in full_file_paths:
  try:
    x=extract_face(i)
    cv2_imshow(x)
    #cv2_imwrite()
    cv2.imwrite(os.path.join(path,"img"+str(count)+".jpg"), x)#writing to path
    count+=1
  except:
    continue

"""There are two ways to group the faces into their individual character .
1)directly download the extracted faces and then manually split it into differnet folders 
2) using KMeans clustering  

**although the KMeans clustering doesnt provide perfect results in out below implemented model , but it reduces the effort of manual split and however there are misclassification in the clustered folders however we can easily detect them and the time taken for split reduces using the KMeans** .
"""

!zip -r /content/file.zip /content/sample_data/face_extracted

from google.colab import files

files.download('/content/file.zip')

"""from matplotlib import pyplot
from matplotlib.patches import Rectangle
from matplotlib.patches import Circle
from mtcnn.mtcnn import MTCNN

#detector=MTCNN()

"""from mtcnn.mtcnn import MTCNN
import cv2

image=cv2.imread(full_file_paths[0])

from google.colab.patches import cv2_imshow
cv2_imshow(image)

detector=MTCNN()
faces=detector.detect_faces(image)
for face in faces:
  print(face)

def create_bb0x(image):
  faces=detector.detect_faces(image)
  bounding_box=faces[0]['box']
  #keypoints=faces[0]['keypoints']

  cv2.rectangle(image,(bounding_box[0],bounding_box[1]),
                (bounding_box[0]+bounding_box[2],bounding_box[1]+bounding_box[3]),
                (0,255,0),
                2)
  return image

marked_img=create_bb0x(image)

cv2_imshow(marked_img)

"""def extract_face(image,resize=(244,244)):
  image=cv2.imread(image)

  faces=detector.detect_faces(image)
  face_images = []

  for face in faces:
    x1,y1,width,height=faces[0]['box']
    x2,y2=x1+width,y1+height

    face_boundary=[]

    face_boundary=image[y1:y2,x1:x2]

    face_image=cv2.resize(face_boundary,resize)

  return face_image

"""path='/content/sample_data/hellno'
count=0
for i in full_file_paths:
  try:
    x=extract_face(i)
    cv2_imshow(x)
    #cv2_imwrite()
    cv2.imwrite(os.path.join(path,"img"+str(count)+".jpg"), x)
    count+=1
  except:
    continue

"""### Labelling

We have already extracted Faces, now we need to group the faces by the characters.

We need to ensure that there enough images for each of the key characters.

Currently, we have extracted faces from only one episode, ideally we should extract from 10-15 shows to find all the potential variations for each of the characters.

Below is the distribution of the label images.

**KMeans implementation below**
"""

def get_paths():
  paths=[os.path.join('/content/sample_data/face_extracted',f)for f in os.listdir('/content/sample_data/face_extracted')]
  return paths
full_file_paths1=get_paths()

import os,shutil,glob
import cv2
import tensorflow as tf
from sklearn.cluster import KMeans
import numpy as np

images =[cv2.resize(cv2.imread(path),(244,244))for path in full_file_paths1]
images =np.array(np.float32(images).reshape(len(images),-1)/255)

model=tf.keras.applications.MobileNetV2(include_top=False,weights='imagenet',input_shape=(244,244,3))
predictions=model.predict(images.reshape(-1,244,244,3))
pred_images=predictions.reshape(images.shape[0],-1)

os.mkdir('output')

input_dir='/content/sample_data/face_extracted'
glob_dir=input_dir + '/*.jpg'
paths=[file for file in glob.glob(glob_dir)]
paths

k=15 """ taking k value as 15 since we have 15 different characters else where 
we can do few transformations and then plot an elow curve and get the value of K""" 
kmodel=KMeans(n_clusters=k,n_jobs=-1,random_state=728)
kmodel.fit(pred_images)
kpredictions=kmodel.predict(pred_images)
shutil.rmtree('output')

for i in range(k):
  #os.makedirs("output\cluster"+str(i))
  os.makedirs(os.path.join(path,"output\cluster"+str(i)))

for i in range(len(paths)):
  shutil.copy2(paths[i],"/content/output/output\cluster"+str(kpredictions[i]))

!zip -r /content/file.zip /content/output

from google.colab import files

files.download('/content/file.zip')

"""Now since we have got the clusters of differnt images we can do few manual partition of the images and renaming of folders , we are doing this procedure since there are few misclassified images in clusters hence to remove that we are going for some manual preprocessing before actual classification."""



"""Now the pre processed data is then used over differnt models and we get the embeddings of each images and then perform classification,we are splitting the data into train and test images."""

#from zipfile import ZipFile
#ZipFile("/content/drive/MyDrive/Copy of sample_data.zip").extractall("/content/sample_data/training")

import numpy as np
import pandas as pd
import cv2
#from mtcnn.mtcnn import MTCNN
from matplotlib import pyplot as plt
from keras.models import load_model
from PIL import Image
import os

"""from tensorflow.keras.preprocessing.image import ImageDataGenerator

"""train_gen=ImageDataGenerator(rescale=1/.255,
                             rotation_range=7,
                             zoom_range=0.2,
                             horizontal_flip=True)

"""train_dataset=train_gen.flow_from_directory('/content/sample_data/training/sample_data',
                                                     target_size=(64,64),batch_size=8,
                                                     class_mode='categorical',
                                                     shuffle=True)

"""train_dataset.class_indices

import os
os.mkdir('/content/sample_data/facenet')
os.mkdir('/content/sample_data/training')
os.mkdir('/content/sample_data/test')

from zipfile import ZipFile
ZipFile("/content/drive/MyDrive/Copy of keras-facenet-20210526T041514Z-001.zip").extractall("/content/sample_data/facenet")

from zipfile import ZipFile
ZipFile("/content/training.zip").extractall("/content/sample_data/training")

from zipfile import ZipFile
ZipFile("/content/drive/MyDrive/Copy of testing.zip").extractall("/content/sample_data/test")

import numpy as np

def extract_f(file_name,required_size=(160,160)):
    image = Image.open(file_name)
    image = image.convert('RGB')
    pixels = np.asarray(image)
    image = Image.fromarray(pixels)
    image = image.resize(required_size)
    face_array = np.asarray(image)
    return face_array



def load_face(dir):
    faces = list()
    for filename in os.listdir(dir):
        path = dir + filename
        face = extract_f(path)
        faces.append(face)
    return faces

def load_dataset(dir):
    X, y = list(), list()
    for subdir in os.listdir(dir):
        path = dir + subdir + '/'
        faces = load_face(path)
        labels = [subdir for i in range(len(faces))]
        print("loaded %d sample for class: %s" % (len(faces),subdir))
        X.extend(faces)
        y.extend(labels)
    return np.asarray(X), np.asarray(y)


trainX, trainy = load_dataset('/content/sample_data/training/training/')
print(trainX.shape, trainy.shape)


testX, testy = load_dataset('/content/sample_data/test/testing/')
print(testX.shape, testy.shape)

"""### Face Recognition

Now, we have the labelled data for us to train the model. There are multiple approaches to build a image classifier model.

We are selecting to convert each of the faces to embedding and then fitting a classifier.

Again for the classification model, we can try a few algoritms- Support Vector Classifier or XGB.
"""

import warnings
warnings.filterwarnings("ignore")

facenet_model = load_model('/content/sample_data/facenet/keras-facenet/model/facenet_keras.h5')

def get_embedding(model, face):
    face = face.astype('float32')
    mean, std = face.mean(), face.std()
    face = (face-mean)/std
    sample = np.expand_dims(face, axis=0)
    yhat = model.predict(sample)
    return yhat[0]

emdTrainX = list()
for face in trainX:
    emd = get_embedding(facenet_model, face)
    emdTrainX.append(emd)
    break
emdTrainX = np.asarray(emdTrainX)

emdTrainX = list()
for face in trainX:
    emd = get_embedding(facenet_model, face)
    emdTrainX.append(emd)
emdTrainX = np.asarray(emdTrainX)
print(emdTrainX.shape)


emdTestX = list()
for face in testX:
    emd = get_embedding(facenet_model, face)
    emdTestX.append(emd)
emdTestX = np.asarray(emdTestX)
print(emdTestX.shape)

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import Normalizer

in_encoder = Normalizer()
emdTrainX_norm = in_encoder.transform(emdTrainX)
emdTestX_norm = in_encoder.transform(emdTestX)
out_encoder = LabelEncoder()
out_encoder.fit(trainy)
trainy_enc = out_encoder.transform(trainy)
testy_enc = out_encoder.transform(testy)

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

model = SVC(kernel='linear', probability=True)
model.fit(emdTrainX_norm, trainy_enc)
yhat_train = model.predict(emdTrainX_norm)
yhat_test = model.predict(emdTestX_norm)
score_train = accuracy_score(trainy_enc, yhat_train)
score_test = accuracy_score(testy_enc, yhat_test)
print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))

"""### Model Validation

We can calculate and compare performance of the model across both Train and Validation samples.

Also, we can extract frames from new episode and validate the performance.
"""

from random import choice

selection = choice([i for i in range(testX.shape[0])])
random_face = testX[selection]
random_face_emd = emdTestX_norm[selection]
random_face_class = testy_enc[selection]
random_face_name = out_encoder.inverse_transform([random_face_class])

samples = np.expand_dims(random_face_emd, axis=0)
yhat_class = model.predict(samples)
print(yhat_class)
yhat_prob = model.predict_proba(samples)
print(yhat_prob)
class_index = yhat_class[0]
class_probability = yhat_prob[0,class_index] * 100
predict_names = out_encoder.inverse_transform(yhat_class)
all_names = out_encoder.inverse_transform([0,1,2,3,4])
print('Predicted: \n%s \n%s' % (all_names, yhat_prob[0]*100))
print('Expected: %s' % random_face_name[0])
plt.imshow(random_face)
title = '%s (%.3f)' % (predict_names[0], class_probability)
plt.title(title)
plt.show()

"""Below is the implementation of classification using Deep learning techniques"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab.patches import cv2_imshow
import zipfile
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization

training_generator = ImageDataGenerator(rescale=1./255,
                                        rotation_range=7,
                                        horizontal_flip=True,
                                        zoom_range=0.2)
train_dataset = training_generator.flow_from_directory('/content/sample_data/training/training',
                                                        target_size = (48, 48),
                                                        batch_size = 16,
                                                        class_mode = 'categorical',
                                                        shuffle = True)

np.unique(train_dataset.classes, return_counts=True)

train_dataset.class_indices

num_detectors = 32
num_classes = 12 # since we have 12 different classes of images
width, height = 48, 48
epochs = 100

network = Sequential()

network.add(Conv2D(num_detectors, (3,3), activation='relu', padding = 'same', input_shape = (width, height, 3)))
network.add(BatchNormalization())
network.add(Conv2D(num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(MaxPooling2D(pool_size=(2,2)))
network.add(Dropout(0.2))

network.add(Conv2D(2*num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(Conv2D(2*num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(MaxPooling2D(pool_size=(2,2)))
network.add(Dropout(0.2))

network.add(Conv2D(2*2*num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(Conv2D(2*2*num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(MaxPooling2D(pool_size=(2,2)))
network.add(Dropout(0.2))

network.add(Conv2D(2*2*2*num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(Conv2D(2*2*2*num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(MaxPooling2D(pool_size=(2,2)))
network.add(Dropout(0.2))

network.add(Flatten())

network.add(Dense(2 * num_detectors, activation='relu'))
network.add(BatchNormalization())
network.add(Dropout(0.2))

network.add(Dense(2 * num_detectors, activation='relu'))
network.add(BatchNormalization())
network.add(Dropout(0.2))

network.add(Dense(num_classes, activation='softmax'))
print(network.summary())

network.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

network.fit(train_dataset, epochs=epochs)





"""# below we are saving the model which we fit into a H5 file for building a model pipeline so we will be using this H5 file and loading the model and then implement it inthe backend and connect it to front end which we will be doing using a flask api and spyder framework"""





from keras.models import load_model

network.save('model_Face.h5')

from keras.models import load_model
network_loaded=load_model("/content/model_Face.h5")

"""after Loading the model we are predicting for single image how ever we have implemeneted after downloading the H5 file in spyder frame work creating an api and classifying the image , the code is shared in a seperate file ."""



network_loaded.summary()

image = cv2.imread('/content/sample_data/training/training/Aditya Aunt/img1010.jpg')
cv2_imshow(image)

image.shape



roi = cv2.resize(image, (48, 48))
cv2_imshow(roi)

roi = roi / 255
roi

roi = np.expand_dims(roi, axis = 0)
roi.shape

probs = network_loaded.predict(roi)
probs

result = np.argmax(probs)
result

"""***Below is the implementation emotion Clssification over the above images***"""

!pip install rmn==3.0.0a4

from rmn import RMN
m=RMN()

import os
os.mkdir('/content/sample_data/emotion_det')

import os
os.mkdir('/content/sample_data/alldata')
os.mkdir('/content/sample_data/alldata/happy')
os.mkdir('/content/sample_data/alldata/sad')
os.mkdir('/content/sample_data/alldata/angry')
os.mkdir('/content/sample_data/alldata/disgust')
os.mkdir('/content/sample_data/alldata/fear')
os.mkdir('/content/sample_data/alldata/suprise')
os.mkdir('/content/sample_data/alldata/neutral')

def emotion_det(image):
  img=cv2.imread(image)
  
  
  results=m.detect_emotion_for_single_face_image(img)
  
  if results[0]=='happy':
    
    path='/content/sample_data/alldata/happy'
    cv2.imwrite(os.path.join(path,"happy"+str(count)+".jpg"), img)
    

  elif results[0]=='angry':
    
    path='/content/sample_data/alldata/angry'
    cv2.imwrite(os.path.join(path,"angry"+str(count)+".jpg"), img)
    
  
  elif results[0]=='sad':
    
    path='/content/sample_data/alldata/sad'
    cv2.imwrite(os.path.join(path,"sad"+str(count)+".jpg"), img)
    
    
  
  elif results[0]=='disgust':
   
    path='/content/sample_data/alldata/disgust'
    cv2.imwrite(os.path.join(path,"disgust"+str(count)+".jpg"), img)
    

  
  elif results[0]=='fear':
 
    path='/content/sample_data/alldata/fear'
    cv2.imwrite(os.path.join(path,"fear"+str(count)+".jpg"), img)
    

  
  elif results[0]=='suprise':
    
    path='/content/sample_data/alldata/suprise'
    cv2.imwrite(os.path.join(path,"suprise"+str(count)+".jpg"), img)
        

  
  elif results[0]=='neutral':
    path='/content/sample_data/alldata/neutral'
    cv2.imwrite(os.path.join(path,"neutral"+str(count)+".jpg"), img)

count=0
for i in full_file_paths1:
  emotion_det(i)
  count+=1

import cv2
image=cv2.imread("/content/sample_data/training/training/Aditya Aunt/img1024.jpg")
#assert image is not None
results=m.detect_emotion_for_single_face_image(image)
print(results)
#results[0]=='happy'

import cv2
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab.patches import cv2_imshow
import zipfile
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization

training_generator = ImageDataGenerator(rescale=1./255,
                                        rotation_range=7,
                                        horizontal_flip=True,
                                        zoom_range=0.2)
train_dataset = training_generator.flow_from_directory('/content/sample_data/alldata/',
                                                        target_size = (48, 48),
                                                        batch_size = 16,
                                                        class_mode = 'categorical',
                                                        shuffle = True)

train_dataset.class_indices

np.unique(train_dataset.classes, return_counts=True)

num_detectors = 32
num_classes = 7 # we have 7 different classes henece the value here is 7
width, height = 48, 48
epochs = 30

network = Sequential()

network.add(Conv2D(num_detectors, (3,3), activation='relu', padding = 'same', input_shape = (width, height, 3)))
network.add(BatchNormalization())
network.add(Conv2D(num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(MaxPooling2D(pool_size=(2,2)))
network.add(Dropout(0.2))

network.add(Conv2D(2*num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(Conv2D(2*num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(MaxPooling2D(pool_size=(2,2)))
network.add(Dropout(0.2))

network.add(Conv2D(2*2*num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(Conv2D(2*2*num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(MaxPooling2D(pool_size=(2,2)))
network.add(Dropout(0.2))

network.add(Conv2D(2*2*2*num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(Conv2D(2*2*2*num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(MaxPooling2D(pool_size=(2,2)))
network.add(Dropout(0.2))

network.add(Flatten())

network.add(Dense(2 * num_detectors, activation='relu'))
network.add(BatchNormalization())
network.add(Dropout(0.2))

network.add(Dense(2 * num_detectors, activation='relu'))
network.add(BatchNormalization())
network.add(Dropout(0.2))

network.add(Dense(num_classes, activation='softmax'))
print(network.summary())

network.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

network.fit(train_dataset, epochs=epochs)

from keras.models import load_model

network.save('emotion_Face.h5')

from keras.models import load_model
network_loaded=load_model("/content/model_Face.h5")

network_loaded.summary()

import cv2

image = cv2.imread('/content/sample_data/training/training/Aditya Aunt/img1010.jpg')
# taking a random image to detect the emotion
cv2_imshow(image)

roi = cv2.resize(image, (48, 48))
cv2_imshow(roi)

roi = roi / 255
roi

roi = np.expand_dims(roi, axis = 0)
roi.shape

probs = network_loaded.predict(roi)
probs

result = np.argmax(probs)
result

class_indices[result]



"""### Model Pipeline and Testing

We can consoldate the steps and build a end to end pipeline. There are a few options

*   Takes image as input and gives image with label & box around face
*   Takes input video and gives output as sequence of frames with right label & box
*   Takes video input and saves outout (json or csv files) to an output folder. Also, we create a staked barchart based on character presence in 5 minutes interval across the episode for top (based on the # of frames labelled) 5 characters



"""















"""### Conclusion and Next Steps

We can highlight the next steps..Adding more labbeled data or increasing episodes

The Below code can be run on pycharm frame work using webcam
"""

import cv2
#from mtcnn.mtcnn import MTCNN
import os
#from PIL import Image
import numpy as np

def get_image_data():
    paths = [os.path.join('zep', f) for f in
             os.listdir('zep')]
    faces = []
    ids = []
    for path in paths:
        imag =cv2.imread(path)
        image=cv2.cvtColor(imag,cv2.COLOR_BGR2GRAY)
        image_np = np.array(image, 'uint8')
        id = int(path.split('.')[1])

        ids.append(id)
        faces.append(image_np)

    return np.array(ids), faces

ids, faces = get_image_data()

lbph_classifier = cv2.face.LBPHFaceRecognizer_create()
lbph_classifier.train(faces, ids)
lbph_classifier.write('lbph_classifier(1).yml')




lbph_face_classifier = cv2.face.LBPHFaceRecognizer_create()
lbph_face_classifier.read('lbph_classifier(1).yml')


face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')


face_recognizer=cv2.face.LBPHFaceRecognizer_create()

face_recognizer.read('lbph_classifier(1).yml')

width,height=224,224

font=cv2.FONT_HERSHEY_COMPLEX_SMALL

video_capture = cv2.VideoCapture(0)


while(True):
  connected,image1=video_capture.read()
  img_gry=cv2.cvtColor(image1,cv2.COLOR_BGR2GRAY)
  detections=face_detector.detectMultiScale(img_gry,scaleFactor=1.5,minSize=(30,30))

  for x,y,w,h in detections:
    img_fac=cv2.resize(img_gry[y:y+w,x:x+h],(width,height))
    cv2.rectangle(image1,(x,y),(x+w,y+h),(0,255,0),2)
    id,confi=face_recognizer.predict(img_fac)
    name=""
    if id==1:
      name='aditya_aunt'
    elif id==2:
      name='Aditya'
    elif id==3:
        name="Amma"
    elif id==4:
        name="Aparna"

    elif id==5:
        name="Dhruv"

    elif id == 6:
        name = "Imlie"

    elif id==7:
        name="police"


    elif id==8:
        name="Rupali"

    elif id==9:
        name="satyakam"

    elif id == 10:
        name = "unknown1"

    elif id==11:
        name="unknown2"
    else:
        name='out of movie'

    font = cv2.FONT_HERSHEY_DUPLEX
    cv2.putText(image1,name,(x,y+(w+30)),font,2,(0,255,0))
    #cv2.putText(image1,confi,(x,y+(h+50)),font,2,(0,0,255))

  cv2.imshow("Face",image1)
  if cv2.waitKey(1)==ord('q'):
    break
video_capture.release()
cv2.destroyAllWindows()

"""This code below implements the same as above but we are reading the video directly and implementing it instead of using the webcam"""

from zipfile import ZipFile
ZipFile("/content/zep1.zip").extractall("/content/sample_data/video_rec")



def get_image_data():
  paths = [os.path.join('/content/sample_data/video_rec/zep1', f) for f in os.listdir('/content/sample_data/video_rec/zep1')]
  faces = []
  ids = []
  for path in paths:
    image = Image.open(path).convert('L')
    image_np = np.array(image, 'uint8')
    id = int(path.split('.')[1])
    
    ids.append(id)
    faces.append(image_np)
  
  return np.array(ids), faces

ids, faces = get_image_data()

lbph_classifier = cv2.face.LBPHFaceRecognizer_create()
lbph_classifier.train(faces, ids)
lbph_classifier.write('lbph_classifier.yml')

lbph_face_classifier = cv2.face.LBPHFaceRecognizer_create()
lbph_face_classifier.read('/content/lbph_classifier.yml')

import cv2
from google.colab.patches import cv2_imshow

face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')


face_recognizer=cv2.face.LBPHFaceRecognizer_create()

face_recognizer.read('/content/lbph_classifier.yml')

width,height=224,224

font=cv2.FONT_HERSHEY_COMPLEX_SMALL

video_capture = cv2.VideoCapture("/content/drive/MyDrive/Copy of Imlie 22nd May 2021.mp4 _0.mp4")


while(True):
  connected,image=video_capture.read()
  img_gry=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)
  detections=face_detector.detectMultiScale(img_gry,scaleFactor=1.5,minSize=(30,30))

  for x,y,w,h in detections:
    img_fac=cv2.resize(img_gry[y:y+w,x:x+h],(width,height))
    cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),2)
    id,confi=face_recognizer.predict(img_fac)
    name=""
    if id==1:
     name='aditya_aunt'
    elif id==2:
      name='Aditya'
    elif id==3:
        name="Amma"
    elif id==4:
        name="Aparna"

    elif id==5:
        name="Dhruv"

    elif id == 6:
        name = "Imlie"

    elif id==7:
        name="police"


    elif id==8:
        name="Rupali"

    elif id==9:
        name="satyakam"

    elif id == 10:
        name = "unknown1"

    elif id==11:
        name="unknown2"
    else:
        name='out of movie'

    cv2.putText(image,name,(x,y+(w+30)),font,2,(0,255,0))  
    cv2.putText(image,confi,(x,y+(h+50)),font,2,(0,0,255))

  cv2_imshow(image)
  if cv2.waitKey(1)==ord('q'):
    break
cv2.destroyAllWindows()

"""Now we will also implement Image Segmentation

refrence: https://www.analyticsvidhya.com/blog/2019/07/computer-vision-implementing-mask-r-cnn-image-segmentation/

https://towardsdatascience.com/computer-vision-instance-segmentation-with-mask-r-cnn-7983502fcad1

Note:
change acceleration from none to Gpu before proceeding 
"""

import warnings
warnings.filterwarnings("ignore")

import os

!git clone https://github.com/matterport/Mask_RCNN

# Commented out IPython magic to ensure Python compatibility.
# %cd Mask_RCNN

pwd

!python setup.py install

# Commented out IPython magic to ensure Python compatibility.
# %cd ..

pwd

import os
import sys
import cv2
import numpy as np
import skimage.io
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow as tf

ROOT_DIR = os.path.abspath('./Mask_RCNN')
ROOT_DIR

sys.path

sys.path.append(ROOT_DIR)

from mrcnn import utils
from mrcnn import visualize
import mrcnn.model as modellib

sys.path.append(os.path.join(ROOT_DIR, 'samples/coco/'))

import coco

MODEL_DIR = os.path.join(ROOT_DIR, 'logs')
IMAGE_DIR = os.path.join(ROOT_DIR, 'images')

COCO_MODEL_PATH = os.path.join(ROOT_DIR, 'mask_rcnn_coco.h5')

utils.download_trained_weights(COCO_MODEL_PATH)

network = modellib.MaskRCNN(mode='inference', model_dir=MODEL_DIR,config=config)

class InferenceConfig(coco.CocoConfig):
  GPU_COUNT = 1
  IMAGES_PER_GPU = 1
config = InferenceConfig()

network.load_weights(COCO_MODEL_PATH, by_name=True)

image2 = cv2.imread('/content/sample_data/image/img69.jpg')
plt.imshow(image2); # BGR

image = skimage.io.imread('/content/sample_data/image/img69.jpg')

results = network.detect([image], verbose=0)
results

r = results[0]

visualize.display_instances(image, r['rois'], r['masks'],
                            r['class_ids'], r['scores'])

np.unique(r['masks'], return_counts=True)

r['masks']

def segment(image, r, index):
  mask = r['masks'][:,:,index]
  #print(mask)
  #print(mask.shape)

  mask = np.stack((mask,) * 3, axis = -1)
  #print(mask)
  #print(mask.shape)

  mask = mask.astype('uint8')
  #print(mask)
  bg = 255 - mask * 255
  #print(mask, mask.min(), mask.max())

  mask_show = np.invert(bg)
  #print(mask_show)
  mask_img = image * mask
  #print(mask_img)

  result = mask_img + bg
  return result, mask_show

segmentation, mask_obj = segment(image, r, 0)

def show_segment(image, r, index, show_mask = False):
  segmentation, mask_obj = segment(image, r, index)
  plt.subplots(1, figsize=(16,16))
  plt.axis('off')
  if show_mask == True:
    plt.imshow(np.concatenate([mask_obj, segmentation], axis = 1))
  else:
    plt.imshow(np.concatenate([image, segmentation], axis = 1))

show_segment(image, r, 1, False)

for index in range(len(r['rois'])):
  show_segment(image,r, index, True)

for index in range(len(r['rois'])):
  show_segment(image,r, index, False)



